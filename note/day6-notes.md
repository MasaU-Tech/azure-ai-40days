# Day6 Notes

## 今日の進捗まとめ
- プロンプト改善を評価する仕組みを実装
- `testset.jsonl` を新規作成し、旧/新プロンプトの比較を実施
- `eval_prompts.py` で **スコア算出 + グラフ生成**
- `analyze_day6.py` で **自然さ・羅列疑惑・Jaccard係数** を検証
- 改善後プロンプトに「必須キーワード」「自然な文」を加えた結果、Base Avg: 0.3 → Improved Avg: 1.0 に向上
- 不自然な羅列は発生せず、改善案が有効であると確認

## つまずき
- JSONDecodeError (BOM付きファイル) → `encoding="utf-8-sig"` で解決
- プロンプト改善効果が出にくい → 制約を強化（必須キーワード・自然な文）

## 学び
- 評価は「感覚」ではなく **固定テストセットで定量評価** するのが重要
- 改善効果を因果的に切り分けるには「同一データでの比較」が必須
- 今回の指標（完全一致率）は単純だが、改善効果を明確に確認できた
- 今後は表記ゆれ対応や埋め込み類似度、人手評価の導入が必要

## 成果物
- `results_day6.csv` : 評価結果ログ
- `prompt_compare.png` : 改善前後の比較グラフ
- `day6-notes.md` : 本ノート

## 次回 (Day7)
- Azure Functions + SignalR を使って、OpenAI応答をリアルタイム配信
- 「評価」から「応用」へステップアップ
