---
title: "【Azure OpenAI 40日】Day5: モデル評価結果の可視化と閾値決定"
emoji: 📊
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Azure", "OpenAI", "PromptFlow", "評価", "可視化"]
published: false
---

# TL;DR
- Day4 で得られた評価結果（rows.csv, summary.csv）を入力として活用  
- Python + matplotlib を使ってスコア分布と質問別スコアを可視化  
- スコア分布から「合格/不合格」を分ける暫定閾値を自動算出  
- 閾値と合格率を `threshold.json` に保存して次のステップで活用できるように整理  

---

# Day5 の目的
Day4 では、Azure OpenAI のモデル出力を **評価フローにかけてスコア化** しました。  
ただし、CSVに出力された数値だけでは、どの質問が弱いのか、全体の傾向はどうか、直感的に理解しづらいです。  

Day5 では以下を行いました。  

1. **スコア分布を可視化** → モデルの全体的な性能の傾向を把握  
2. **質問別スコアを可視化** → どの質問が弱点なのかを確認  
3. **閾値を暫定決定** → 合格/不合格を分けるラインを明確化  
4. **結果をJSON保存** → 次のステップ（再評価・改善検討）で活用できる形式に変換  

---

# 実施した手順

## 1. Day4 の成果物を利用
Day5 では、Day4で作成した以下のファイルを利用しました。

- `day4/outputs/rows.csv` … 個別質問ごとの評価結果  
- `day4/outputs/summary.csv` … 平均スコアやNG数などの集計結果  

👉 **コピー不要**。Day5 のスクリプトは `../day4/outputs/` を直接参照しました。

---

## 2. 可視化スクリプトの作成
`day5/viz_scores.py` を作成し、以下の処理を実装しました。

- スコア分布のヒストグラムを生成（`score_hist.png`）  
- 質問ごとの平均スコアを棒グラフ化（`score_by_question.png`）  
- 分布から閾値を暫定決定し `threshold.json` に保存  

---

## 3. 実行結果

### スコア分布
モデルが「高得点に偏っているのか／ばらついているのか」が一目でわかります。  

<!-- スクリーンショット差し込み位置 -->
![Score Distribution](/images/day5/score_hist.png)

---

### 質問別の平均スコア
弱い質問カテゴリを可視化できます。  

<!-- スクリーンショット差し込み位置 -->
![Average Score by Question](/images/day5/score_by_question.png)

---

### 閾値と合格率
最終的に `threshold.json` に以下が出力されました。

```json
{
  "threshold": 0.7,
  "pass_rate": 0.333
}
````

---

# Day5 の学び

* **数値を可視化するだけで理解度が一気に上がる**
* 「どの質問が苦手か」が見えると、改善の方向性が見えやすい
* 閾値の自動計算ルールを導入することで、今後の評価基準を標準化できる

---

# 次回（Day6）の予定

Day6 では、Day5で得られた可視化・閾値を基に **改善方法の検討やモデル設定のチューニング** に入ります。
